#得到的结果就是这封邮件被分为垃圾邮件的概率，值很小
if(length(msg.match) < 1)
{
return(prior * c ^ (length(msg.freq)))
}
#如果不为空集，我们首先要找到同时在训练集和新邮件中出现的特征词，用match匹配，
#然后查出他们在训练集文档中出现的概率（即这类文档在所有文档中出现的概率），
#放在match.probs()中；用prod(match.probs)计算这些返回值的乘积
#然后与邮件为垃圾邮件的先验概率，特征词出现的概率以及未出现在训练集中的词项的小概率
#获得的结果就是已知邮件中有哪些词出现在训练集中后，对于它是垃圾邮件的贝叶斯估计值
else
{
match.probs <- training.df$occurrence[match(msg.match, training.df$term)]
return(prior * prod(match.probs) * c ^ (length(msg.freq) - length(msg.match)))
}
}
# With all of our support functions written, we can perform the classification.
# First, we create document corpus for spam messages
# Get all the SPAM-y email into a single vector
################
#要构建训练分类器就要从垃圾邮件和正常邮件中得到邮件正文
#创建一个向量保存所有邮件正文，从而使向量的每个元素就是一封邮件的正文
###################################################################################
#构建垃圾文件训练集，垃圾文件路径在spam.path中的所有文件
spam.docs <- dir(spam.path)
#因为所有保存邮件数据的路径下，都有一个cmds文件，这个文件包含一个很长的UNIX基本命令列表
#用于在这些目录下移动文件，因为不希望把cmds文件包含在训练数据中，故将其忽略
#此时spam.docs包含所有用于训练分类器的垃圾邮件文件名
spam.docs <- spam.docs[which(spam.docs != "cmds")]
#sapply()对每一个垃圾文件名应用get.msg()函数，返回一个向量，
#每返回一个元素对应spam.docs的每一个名称元素
#本文路径getwd()为（"E:/workspace/R/machine learning/03-Classification"）
#在这个路径中有data/spam 这个目录
#所有邮件都在这个目录中，spam.docs取得了所有文件名，所以要将文件名与路径粘贴在一起
#构造每个邮件文件的路径，有反斜杠/连接
#
all.spam <- sapply(spam.docs,
function(p) get.msg(paste(spam.path, p, sep = "/")))
#也可以用，get.msg(file.path(spam.path, p)))
# Create a DocumentTermMatrix from that vector
#得到语料库
spam.tdm <- get.tdm(all.spam)
# Create a data frame that provides the feature set from the training SPAM data
#将TDM对象转化为matrix（R的标准矩阵）
spam.matrix <- as.matrix(spam.tdm)
#计算每个特征词（每行）在所有文档（一列代表一个邮件）中出现的总频数
spam.counts <- rowSums(spam.matrix)
#将词和每个词出现的频率整理成数据框
#频数可能是字符串形式，因此要将其转化为因子形式
spam.df <- data.frame(cbind(names(spam.counts),
as.numeric(spam.counts)),
stringsAsFactors = FALSE)
names(spam.df) <- c("term", "frequency")
#然后将频数转化为数值类型
spam.df$frequency <- as.numeric(spam.df$frequency)
#############################################
#通过下列两步生成关键的训练数据
#1 计算一个特征词项所出现的文档（包含该词的文档的数量）
#在所有文档（有些文档不含该词）中所占的比例
spam.occurrence <- sapply(1:nrow(spam.matrix),
function(i)
{
length(which(spam.matrix[i, ] > 0)) / ncol(spam.matrix)
})
#计算每个词在所有词中的频率
spam.density <- spam.df$frequency / sum(spam.df$frequency)
# Add the term density and occurrence rate
#用transform()将后两个向量加入到spam.df中
spam.df <- transform(spam.df,
density = spam.density,
occurrence = spam.occurrence)
###########################################################################
# Now do the same for the EASY HAM email
#现在构造正常邮件的训练数据，语料来自easyham.path目录中
#操作方式同 垃圾邮件一样
easyham.docs <- dir(easyham.path)
easyham.docs <- easyham.docs[which(easyham.docs != "cmds")]
all.easyham <- sapply(easyham.docs[1:length(spam.docs)],
function(p) get.msg(file.path(easyham.path, p)))
easyham.tdm <- get.tdm(all.easyham)
easyham.matrix <- as.matrix(easyham.tdm)
easyham.counts <- rowSums(easyham.matrix)
easyham.df <- data.frame(cbind(names(easyham.counts),
as.numeric(easyham.counts)),
stringsAsFactors = FALSE)
names(easyham.df) <- c("term", "frequency")
easyham.df$frequency <- as.numeric(easyham.df$frequency)
easyham.occurrence <- sapply(1:nrow(easyham.matrix),
function(i)
{
length(which(easyham.matrix[i, ] > 0)) / ncol(easyham.matrix)
})
easyham.density <- easyham.df$frequency / sum(easyham.df$frequency)
easyham.df <- transform(easyham.df,
density = easyham.density,
occurrence = easyham.occurrence)
head(easyham.df)
##########################################################################
# 上面我们有垃圾邮件和正常邮件作为训练集，下面再对不易识别的正常邮件 进行分类
# Run classifer against HARD HAM
hardham.docs <- dir(hardham.path)
hardham.docs <- hardham.docs[which(hardham.docs != "cmds")]
#计算每封新邮件是垃圾邮件的概率
hardham.spamtest <- sapply(hardham.docs,
function(p) classify.email(file.path(hardham.path, p), training.df = spam.df))
#计算每封新邮件是正常邮件的概率
hardham.hamtest <- sapply(hardham.docs,
function(p) classify.email(file.path(hardham.path, p), training.df = easyham.df))
hardham.res <- ifelse(hardham.spamtest > hardham.hamtest,
TRUE,
FALSE)
summary(hardham.res)
all.spam <- sapply(spam.docs,
function(p) get.msg(file.path(spam.path, p)))
all.spam
spam.docs
all.spam <- sapply(spam.docs[1:5],
function(p) get.msg(file.path(spam.path, p)))
all.spam
spam.docs <- dir(spam2.path)
spam.docs <- spam.docs[which(spam.docs != "cmds")]
all.spam <- sapply(spam.docs,
function(p) get.msg(file.path(spam2.path, p)))
all.spam <- sapply(spam.docs[1:10],
function(p) get.msg(file.path(spam.path, p)))
spam.docs <- dir(spam.path)
spam.docs <- spam.docs[which(spam.docs != "cmds")]
all.spam <- sapply(spam.docs[1:10],
function(p) get.msg(file.path(spam.path, p)))
library('tm')
library('ggplot2')
# Set the global paths
spam.path <- file.path("data", "spam")
spam2.path <- file.path("data", "spam_2")
easyham.path <- file.path("data", "easy_ham")
easyham2.path <- file.path("data", "easy_ham_2")
hardham.path <- file.path("data", "hard_ham")
hardham2.path <- file.path("data", "hard_ham_2")
get.msg <- function(path)
{
#非ASCII编码字符，指编码定用latin1读取
#path = paste(spam.path, spam.docs[35], sep = "/")
con <- file(path, open = "rt", encoding = "latin1")
#每一行返回一个字符串向量的一个元素
text <- readLines(con)
# The message always begins after the first full line break
#读入了所有文本就要定位到第一空行，然后抽取其后的所有文本
msg <- text[seq(which(text == "")[1] + 1, length(text), 1)]
#关闭文件
close(con)
#将向量拼接成一个单条文本元素，同时指定 \n 换行符作为分隔各个元素
return(paste(msg, collapse = "\n"))
}
# Create a TermDocumentMatrix (TDM) from the corpus of SPAM email.
# The TDM control can be modified, and the sparsity level can be
# altered.  This TDM is used to create the feature set used to do
# train our classifier.
#量化垃圾邮件特征词项频率的方法之一是构造一个词项文档矩阵TMD
#该矩阵行对应特定语料库的所有文档中抽取的词项
#列对应语料库中所有的文档
#[i,j]位置元素表示某个词项i在文档j中出现的次数
#该函数输出文本向量，输出TDM
get.tdm <- function(doc.vec)
{
#创建一个TDM之前，要告诉tm该如何清洗和规整文本
#要用到control变量，用于设定如何提取文本
#stopwords = TRUE告诉tm在所有文本中剔除488个常用英文停词，用stopwords()可以查看停词
#removePunctuation = TRUE移除标点符号
#removeNumbers = TRUE移除数字
#minDocFreq = 2确保那些只有在文本中大于或等于2的词才能出现在TDM中
control <- list(stopwords = TRUE,
removePunctuation = TRUE,
removeNumbers = TRUE,
minDocFreq = 2)
#tm包中提供了若干构建语料库(corpus对象)
#用VectorSource()函数构建source对象（数据源类型是corpus对象参数类型source）
#用getsource获得数据源类型
#Corpus()函数与VectorSource()函数一起使用创建一个Corpus对象(语料库对象)
doc.corpus <- Corpus(VectorSource(doc.vec))
#
doc.dtm <- TermDocumentMatrix(doc.corpus, control)
return(doc.dtm)
}
# This function takes a file path to an email file and a string,
# the term parameter, and returns the count of that term in
# the email body.
count.word <- function(path, term)
{
msg <- get.msg(path)
msg.corpus <- Corpus(VectorSource(msg))
# Hard-coded TDM control
control <- list(stopwords = TRUE,
removePunctuation = TRUE,
removeNumbers = TRUE)
msg.tdm <- TermDocumentMatrix(msg.corpus, control)
word.freq <- rowSums(as.matrix(msg.tdm))
term.freq <- word.freq[which(names(word.freq) == term)]
# We use ifelse here because term.freq = NA if nothing is found
return(ifelse(length(term.freq) > 0, term.freq, 0))
}
# This is the our workhorse function for classifying email.  It takes
# two required paramters: a file path to an email to classify, and
# a data frame of the trained data.  The function also takes two
# optional parameters.  First, a prior over the probability that an email
# is SPAM, which we set to 0.5 (naive), and constant value for the
# probability on words in the email that are not in our training data.
# The function returns the naive Bayes probability that the given email
# is SPAM.
#假设每个邮件有垃圾和正常邮件的先验概率相等都为0.5，
#在训练集中没有观测到的词项特征，不代表这些词永远不出现，因此给他们的先验概率为1e-6
#这里构建了正常和垃圾邮件可变的先验概率
classify.email <- function(path, training.df, prior = 0.5, c = 1e-6)
{
# Here, we use many of the support functions to get the
# email text data in a workable format
#前三步和我们构建训练集阶段所做事实一样的
#用get.msg()取得邮件正文;get.tdm()将正文转化为TDM;rowSums()计算每个词出现的频数
msg <- get.msg(path)
msg.tdm <- get.tdm(msg)
msg.freq <- rowSums(as.matrix(msg.tdm))
# Find intersections of words
#接下来找到出现在新邮件中的词项与训练集中出现的词项的交集
#msg.match保存这封邮件中的所有在训练集training.df中出现过的特征词项
msg.match <- intersect(names(msg.freq), training.df$term)
# Now, we just perform the naive Bayes calculation
#如果交集为空，则msg.match长度小于1，先验概率乘以小概率值c的邮件特征数次幂
#得到的结果就是这封邮件被分为垃圾邮件的概率，值很小
if(length(msg.match) < 1)
{
return(prior * c ^ (length(msg.freq)))
}
#如果不为空集，我们首先要找到同时在训练集和新邮件中出现的特征词，用match匹配，
#然后查出他们在训练集文档中出现的概率（即这类文档在所有文档中出现的概率），
#放在match.probs()中；用prod(match.probs)计算这些返回值的乘积
#然后与邮件为垃圾邮件的先验概率，特征词出现的概率以及未出现在训练集中的词项的小概率
#获得的结果就是已知邮件中有哪些词出现在训练集中后，对于它是垃圾邮件的贝叶斯估计值
else
{
match.probs <- training.df$occurrence[match(msg.match, training.df$term)]
return(prior * prod(match.probs) * c ^ (length(msg.freq) - length(msg.match)))
}
}
# With all of our support functions written, we can perform the classification.
# First, we create document corpus for spam messages
# Get all the SPAM-y email into a single vector
################
#要构建训练分类器就要从垃圾邮件和正常邮件中得到邮件正文
#创建一个向量保存所有邮件正文，从而使向量的每个元素就是一封邮件的正文
spam.docs <- dir(spam.path)
spam.docs <- spam.docs[which(spam.docs != "cmds")]
all.spam <- sapply(spam.docs[1:10],
function(p) get.msg(file.path(spam.path, p)))
all.spam <- sapply(spam.docs[1:5],
function(p) get.msg(file.path(spam.path, p)))
all.spam <- sapply(spam.docs[-c(6, 9, 35)],
function(p) get.msg(file.path(spam.path, p)))
all.spam <- sapply(spam.docs[10:34],
function(p) get.msg(file.path(spam.path, p)))
all.spam <- sapply(spam.docs[-c(6, 9, 35)],
function(p) get.msg(file.path(spam.path, p)))
path = paste(spam.path, spam.docs[35], sep = "/")
path = paste(spam.path, spam.docs[35], sep = "/")
con <- file(path, open = "rt", encoding = "latin1")
text <- readLines(con)
con
readLines(con, skipNul = FALSE)
text <- readLines(con, skipNul = FALSE)
text
msg <- text[seq(which(text == "")[1] + 1, length(text), 1)]
path = paste(spam.path, spam.docs[5], sep = "/")
con <- file(path, open = "rt", encoding = "latin1")
text <- readLines(con, skipNul = FALSE)
text
path = paste(spam.path, spam.docs[35], sep = "/")
con <- file(path, open = "rt", encoding = "latin1")
text <- readLines(con, skipNul = FALSE)
path = paste(spam.path, spam.docs[35], sep = "/")
con <- file(path, open = "rt", encoding = "latin1")
text <- readLines(con, skipNul = FALSE)
if(length(text) != 0)
msg <- text[seq(which(text == "")[1] + 1, length(text), 1)]
#关闭文件
close(con)
#将向量拼接成一个单条文本元素，同时指定 \n 换行符作为分隔各个元素
return(paste(msg, collapse = "\n"))
while(length(text) = 0){
#非ASCII编码字符，指编码定用latin1读取
#path = paste(spam.path, spam.docs[35], sep = "/")
con <- file(path, open = "rt", encoding = "latin1")
#每一行返回一个字符串向量的一个元素
text <- readLines(con, skipNul = FALSE)
}
text = 0
while(nchar(text) = 0){
#非ASCII编码字符，指编码定用latin1读取
#path = paste(spam.path, spam.docs[35], sep = "/")
con <- file(path, open = "rt", encoding = "latin1")
#每一行返回一个字符串向量的一个元素
text <- readLines(con, skipNul = FALSE)
}
path = paste(spam.path, spam.docs[35], sep = "/")
con <- file(path, open = "rt", encoding = "latin1")
text <- readLines(con, skipNul = FALSE)
text
library('tm')
library('ggplot2')
# Set the global paths
spam.path <- file.path("data", "spam")
spam2.path <- file.path("data", "spam_2")
easyham.path <- file.path("data", "easy_ham")
easyham2.path <- file.path("data", "easy_ham_2")
hardham.path <- file.path("data", "hard_ham")
hardham2.path <- file.path("data", "hard_ham_2")
path = paste(spam.path, spam.docs[35], sep = "/")
spam.docs <- dir(spam.path)
spam.docs <- spam.docs[which(spam.docs != "cmds")]
path = paste(spam.path, spam.docs[35], sep = "/")
con <- file(path, open = "rt", encoding = "latin1")
text <- readLines(con, skipNul = FALSE)
text
nchar(text)
path = paste(spam.path, spam.docs[5], sep = "/")
con <- file(path, open = "rt", encoding = "latin1")
text <- readLines(con, skipNul = FALSE)
text
library('tm')
library('ggplot2')
# Set the global paths
spam.path <- file.path("data", "spam")
spam2.path <- file.path("data", "spam_2")
easyham.path <- file.path("data", "easy_ham")
easyham2.path <- file.path("data", "easy_ham_2")
hardham.path <- file.path("data", "hard_ham")
hardham2.path <- file.path("data", "hard_ham_2")
get.msg <- function(path)
{
#非ASCII编码字符，指编码定用latin1读取
#path = paste(spam.path, spam.docs[5], sep = "/")
con <- file(path, open = "rt", encoding = "latin1")
#每一行返回一个字符串向量的一个元素
text <- readLines(con, skipNul = FALSE)
# The message always begins after the first full line break
#读入了所有文本就要定位到第一空行，然后抽取其后的所有文本
msg <- text[seq(which(text == "")[1] + 1, length(text), 1)]
#关闭文件
close(con)
#将向量拼接成一个单条文本元素，同时指定 \n 换行符作为分隔各个元素
return(paste(msg, collapse = "\n"))
}
get.tdm <- function(doc.vec)
{
#创建一个TDM之前，要告诉tm该如何清洗和规整文本
#要用到control变量，用于设定如何提取文本
#stopwords = TRUE告诉tm在所有文本中剔除488个常用英文停词，用stopwords()可以查看停词
#removePunctuation = TRUE移除标点符号
#removeNumbers = TRUE移除数字
#minDocFreq = 2确保那些只有在文本中大于或等于2的词才能出现在TDM中
control <- list(stopwords = TRUE,
removePunctuation = TRUE,
removeNumbers = TRUE,
minDocFreq = 2)
#tm包中提供了若干构建语料库(corpus对象)
#用VectorSource()函数构建source对象（数据源类型是corpus对象参数类型source）
#用getsource获得数据源类型
#Corpus()函数与VectorSource()函数一起使用创建一个Corpus对象(语料库对象)
doc.corpus <- Corpus(VectorSource(doc.vec))
#
doc.dtm <- TermDocumentMatrix(doc.corpus, control)
return(doc.dtm)
}
# This function takes a file path to an email file and a string,
# the term parameter, and returns the count of that term in
# the email body.
count.word <- function(path, term)
{
msg <- get.msg(path)
msg.corpus <- Corpus(VectorSource(msg))
# Hard-coded TDM control
control <- list(stopwords = TRUE,
removePunctuation = TRUE,
removeNumbers = TRUE)
msg.tdm <- TermDocumentMatrix(msg.corpus, control)
word.freq <- rowSums(as.matrix(msg.tdm))
term.freq <- word.freq[which(names(word.freq) == term)]
# We use ifelse here because term.freq = NA if nothing is found
return(ifelse(length(term.freq) > 0, term.freq, 0))
}
# This is the our workhorse function for classifying email.  It takes
# two required paramters: a file path to an email to classify, and
# a data frame of the trained data.  The function also takes two
# optional parameters.  First, a prior over the probability that an email
# is SPAM, which we set to 0.5 (naive), and constant value for the
# probability on words in the email that are not in our training data.
# The function returns the naive Bayes probability that the given email
# is SPAM.
#假设每个邮件有垃圾和正常邮件的先验概率相等都为0.5，
#在训练集中没有观测到的词项特征，不代表这些词永远不出现，因此给他们的先验概率为1e-6
#这里构建了正常和垃圾邮件可变的先验概率
classify.email <- function(path, training.df, prior = 0.5, c = 1e-6)
{
# Here, we use many of the support functions to get the
# email text data in a workable format
#前三步和我们构建训练集阶段所做事实一样的
#用get.msg()取得邮件正文;get.tdm()将正文转化为TDM;rowSums()计算每个词出现的频数
msg <- get.msg(path)
msg.tdm <- get.tdm(msg)
msg.freq <- rowSums(as.matrix(msg.tdm))
# Find intersections of words
#接下来找到出现在新邮件中的词项与训练集中出现的词项的交集
#msg.match保存这封邮件中的所有在训练集training.df中出现过的特征词项
msg.match <- intersect(names(msg.freq), training.df$term)
# Now, we just perform the naive Bayes calculation
#如果交集为空，则msg.match长度小于1，先验概率乘以小概率值c的邮件特征数次幂
#得到的结果就是这封邮件被分为垃圾邮件的概率，值很小
if(length(msg.match) < 1)
{
return(prior * c ^ (length(msg.freq)))
}
#如果不为空集，我们首先要找到同时在训练集和新邮件中出现的特征词，用match匹配，
#然后查出他们在训练集文档中出现的概率（即这类文档在所有文档中出现的概率），
#放在match.probs()中；用prod(match.probs)计算这些返回值的乘积
#然后与邮件为垃圾邮件的先验概率，特征词出现的概率以及未出现在训练集中的词项的小概率
#获得的结果就是已知邮件中有哪些词出现在训练集中后，对于它是垃圾邮件的贝叶斯估计值
else
{
match.probs <- training.df$occurrence[match(msg.match, training.df$term)]
return(prior * prod(match.probs) * c ^ (length(msg.freq) - length(msg.match)))
}
}
# With all of our support functions written, we can perform the classification.
# First, we create document corpus for spam messages
# Get all the SPAM-y email into a single vector
################
#要构建训练分类器就要从垃圾邮件和正常邮件中得到邮件正文
#创建一个向量保存所有邮件正文，从而使向量的每个元素就是一封邮件的正文
###################################################################################
#构建垃圾文件训练集，垃圾文件路径在spam.path中的所有文件
spam.docs <- dir(spam.path)
#因为所有保存邮件数据的路径下，都有一个cmds文件，这个文件包含一个很长的UNIX基本命令列表
#用于在这些目录下移动文件，因为不希望把cmds文件包含在训练数据中，故将其忽略
#此时spam.docs包含所有用于训练分类器的垃圾邮件文件名
spam.docs <- spam.docs[which(spam.docs != "cmds")]
sapply(spam.docs[-35],
function(p) get.msg(file.path(spam.path, p)))
sapply(spam.docs[-c(1:35)],
function(p) get.msg(file.path(spam.path, p)))
length(spam.docs)
sapply(spam.docs[100:500],
function(p) get.msg(file.path(spam.path, p)))
sapply(spam.docs[100:200],
function(p) get.msg(file.path(spam.path, p)))
all.spam <- sapply(spam.docs[100:400],
function(p) get.msg(file.path(spam.path, p)))
View(classify.email)
#
all.spam <- sapply(spam.docs[100:300],
function(p) get.msg(file.path(spam.path, p)))
all.spam <- sapply(spam.docs[100:200],
function(p) get.msg(file.path(spam.path, p)))
spam.tdm <- get.tdm(all.spam)
spam.matrix <- as.matrix(spam.tdm)
#计算每个特征词（每行）在所有文档（一列代表一个邮件）中出现的总频数
spam.counts <- rowSums(spam.matrix)
spam.df <- data.frame(cbind(names(spam.counts),
as.numeric(spam.counts)),
stringsAsFactors = FALSE)
names(spam.df) <- c("term", "frequency")
#然后将频数转化为数值类型
spam.df$frequency <- as.numeric(spam.df$frequency)
#在所有文档（有些文档不含该词）中所占的比例
spam.occurrence <- sapply(1:nrow(spam.matrix),
function(i)
{
length(which(spam.matrix[i, ] > 0)) / ncol(spam.matrix)
})
#计算每个词在所有词中的频率
spam.density <- spam.df$frequency / sum(spam.df$frequency)
spam.df <- transform(spam.df,
density = spam.density,
occurrence = spam.occurrence)
easyham.docs <- dir(easyham.path)
easyham.docs <- easyham.docs[which(easyham.docs != "cmds")]
all.easyham <- sapply(easyham.docs[1:length(spam.docs)],
function(p) get.msg(file.path(easyham.path, p)))
easyham.tdm <- get.tdm(all.easyham)
easyham.matrix <- as.matrix(easyham.tdm)
easyham.counts <- rowSums(easyham.matrix)
easyham.df <- data.frame(cbind(names(easyham.counts),
as.numeric(easyham.counts)),
stringsAsFactors = FALSE)
names(easyham.df) <- c("term", "frequency")
easyham.df$frequency <- as.numeric(easyham.df$frequency)
easyham.occurrence <- sapply(1:nrow(easyham.matrix),
function(i)
{
length(which(easyham.matrix[i, ] > 0)) / ncol(easyham.matrix)
})
easyham.density <- easyham.df$frequency / sum(easyham.df$frequency)
easyham.df <- transform(easyham.df,
density = easyham.density,
occurrence = easyham.occurrence)
hardham.docs <- dir(hardham.path)
hardham.docs <- hardham.docs[which(hardham.docs != "cmds")]
hardham.spamtest <- sapply(hardham.docs,
function(p) classify.email(file.path(hardham.path, p), training.df = spam.df))
#计算每封新邮件是正常邮件的概率
hardham.hamtest <- sapply(hardham.docs,
function(p) classify.email(file.path(hardham.path, p), training.df = easyham.df))
hardham.res <- ifelse(hardham.spamtest > hardham.hamtest,
TRUE,
FALSE)
summary(hardham.res)
