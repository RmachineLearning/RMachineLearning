temp <- getURL("http://www.dataguru.cn/",debugfunction=d$update,verbose =TRUE)
d = debugGatherer()
temp <- getURL("http://www.dataguru.cn/",debugfunction=d$update,verbose =TRUE)
cat(d$value()[3])
cat(d$value()[1])
cat(d$value()[2])
headers = basicTextGatherer()
txt=getURL("http://www.dataguru.cn/",headerfunction = headers$update)
names(headers$value())#说明是字符串形式
headers$value()
h = basicHeaderGatherer()
txtt=getURL("http://www.dataguru.cn/",headerfunction = h$update)
names(h$value())
names(headers$value())
h$value()
curl = getCurlHandle()
d=getURL("http://www.dataguru.cn/", curl = curl)
getCurlInfo(curl)$response.code
getCurlInfo(curl)
url = getURL("http://rfunction.com/code/1202/")
temp<- getBinaryURL(url)
wp = getURL("http://www.bioguo.org/AnimalTFDB/BrowseAllTF.php?spe=Mus_musculus")
doc <- htmlParse(wp, asText = TRUE)
library(XML)
doc <- htmlParse(wp, asText = TRUE)
tables <- readHTMLTable(doc)
tables
head(tables)
str(tables)
tables$table1
head(tables$table1
)
head(tables$table1)
dizhen = getURL("http://data.earthquake.cn/datashare/datashare_more_quickdata_new.jsp")
dizhen_data <- getURL(dizhen)
doc <-htmlParse(wp, asText = TRUE)
tables <- readHTMLTable(doc,header=F)
head(tables)
head(tables$table1)
dizhen_doc <-htmlParse(dizhen, asText = TRUE)
dizhen_tables <- readHTMLTable(dizhen_doc,header=F)
head(dizhen_tables)
install.packages("RExcelInstaller", "rcom", "rsproxy")
require(devtools)
install.packages(devtools)
install.packages(“httr”)
install.packages("httr")
install.packages("devtools")
install_github("ramnathv/rCharts")
install_github("rcharts/ramnathv")
require(devtools)
find_rtools()
install.packages(c("abind", "adabag", "AER", "amap", "arules", "bayesm", "bdsmatrix", "BH", "boot", "bootstrap", "BradleyTerry2", "BRugs", "cairoDevice", "car", "caret", "caTools", "chron", "class", "classInt", "cluster", "coda", "colorspace", "corpcor", "corrgram", "coxme", "DCluster", "deldir", "DEoptimR", "deSolve", "diagram", "digest", "doBy", "dplyr", "dse", "e1071", "Ecdat", "Ecfun", "effects", "emplik", "evaluate", "fBasics", "fda", "fields", "forecast", "foreign", "formatR", "Formula", "gam", "gdata", "gee", "geoR", "geosphere", "GGally", "ggm", "ggmap", "ggplot2", "glmnet", "goftest", "GPArotation", "gplots", "gss", "gstat", "gtools", "gWidgetsRGtk2", "HH", "highr", "Hmisc", "htmltools", "httpuv", "igraph", "intervals", "ipred", "JM", "JMbayes", "kernlab", "KernSmooth", "kknn", "knitr", "kohonen", "labeling", "lava", "lme4", "lmtest", "manipulate", "mapdata", "mapproj", "maps", "maptools", "markdown", "MASS", "Matrix", "mboost", "mfp", "mime", "minqa", "mnormt", "MSBVAR", "multcomp", "mutoss", "mvoutlier", "nlme", "NLP", "nnet", "numDeriv", "party", "pcaPP", "pgirmess", "plotrix", "plspm", "plyr", "polyclip", "prodlim", "pscl", "psych", "pwr", "quantmod", "R.matlab", "R.methodsS3", "R.oo", "R.utils", "R2WinBUGS", "R6", "RandomFields", "randomForest", "RANN", "raster", "Rcmdr", "RColorBrewer", "Rcpp", "RcppArmadillo", "RcppEigen", "RCurl", "reshape2", "rgdal", "rgeos", "rgl", "RgoogleMaps", "rjson", "rmarkdown", "rminer", "robCompositions", "robustbase", "ROCR", "RODBC", "rpart", "rpart.plot", "rrcov", "sandwich", "scales", "sem", "seriation", "setRNG", "shape", "shiny", "sp", "spacetime", "spam", "SparseM", "spatial", "spatstat", "spBayes", "spdep", "sphet", "splancs", "splm", "stabledist", "statmod", "stringr", "strucchange", "survival", "systemfit", "tcltk2", "tfplot", "tframe", "TH.data", "timeDate", "timeSeries", "tm", "tsDyn", "tseries", "TSP", "TTR", "vcd", "VIM", "xlsxjars", "XML", "xtable", "zoo"))
install.packages(c("abind", "adabag", "AER", "amap", "arules",
install.packages(c("abind", "adabag", "AER", "amap", "arules", "bayesm", "bdsmatrix", "BH", "boot", "bootstrap", "BradleyTerry2", "BRugs", "cairoDevice", "car", "caret", "caTools", "chron", "class", "classInt", "cluster", "coda", "colorspace", "corpcor", "corrgram", "coxme", "DCluster", "deldir", "DEoptimR", "deSolve", "diagram", "digest", "doBy", "dplyr", "dse", "e1071", "Ecdat", "Ecfun", "effects", "emplik", "evaluate", "fBasics", "fda", "fields", "forecast", "foreign", "formatR", "Formula", "gam", "gdata", "gee", "geoR", "geosphere", "GGally", "ggm", "ggmap", "ggplot2", "glmnet", "goftest", "GPArotation", "gplots", "gss", "gstat", "gtools", "gWidgetsRGtk2", "HH", "highr", "Hmisc", "htmltools", "httpuv", "igraph", "intervals", "ipred", "JM", "JMbayes", "kernlab", "KernSmooth", "kknn", "knitr", "kohonen", "labeling", "lava", "lme4", "lmtest", "manipulate", "mapdata", "mapproj", "maps", "maptools", "markdown", "MASS", "Matrix", "mboost", "mfp", "mime", "minqa", "mnormt", "MSBVAR", "multcomp", "mutoss", "mvoutlier", "nlme", "NLP", "nnet", "numDeriv", "party", "pcaPP", "pgirmess", "plotrix", "plspm", "plyr", "polyclip", "prodlim", "pscl", "psych", "pwr", "quantmod", "R.matlab", "R.methodsS3", "R.oo", "R.utils", "R2WinBUGS", "R6", "RandomFields", "randomForest", "RANN", "raster", "Rcmdr", "RColorBrewer", "Rcpp", "RcppArmadillo", "RcppEigen", "RCurl", "reshape2", "rgdal", "rgeos", "rgl", "RgoogleMaps", "rjson", "rmarkdown", "rminer", "robCompositions", "robustbase", "ROCR", "RODBC", "rpart", "rpart.plot", "rrcov", "sandwich", "scales", "sem", "seriation", "setRNG", "shape", "shiny", "sp", "spacetime", "spam", "SparseM", "spatial", "spatstat", "spBayes", "spdep", "sphet", "splancs", "splm", "stabledist", "statmod", "stringr", "strucchange", "survival", "systemfit", "tcltk2", "tfplot", "tframe", "TH.data", "timeDate", "timeSeries", "tm", "tsDyn", "tseries", "TSP", "TTR", "vcd", "VIM", "xlsxjars", "XML", "xtable", "zoo"))
install.packages(c("abind", "adabag", "AER", "amap", "arules",
library("abind", lib.loc="E:/Program Files/R/R-3.2.1/library")
install.packages("dse")
install.packages("GPArotation")
install.packages(c("timeDate", "timeSeries", "tseries"))
library("dplyr", lib.loc="E:/Program Files/R/R-3.2.1/library")
lbrary(dplyr)
library(dplyr)
library("plyr", lib.loc="E:/Program Files/R/R-3.2.1/library")
xx<- array(1:24,c(3,4,2))
a<-array(1:21,c(3,7))
require(plyr)
library(plyr)
aaply(.data=a, .margins=1, .fun=mean)
a
xx
aaply(.data=a,  2, mean,  process="text")
names=c("john","mary","Alice","Peter","ROger","Phyillis")
age=c(13,15,14,13,14,13)
sex=c("M","F","F","M","M","F")
data=data.frame(names,age,sex)
aaply(.data=xx, .margins=1, .fun=mean)
aaply(.data=xx,  .margins = 2, .fun=mean,  process="text")
amean=function(data)
{
agemean=mean(data[,2])
return(agemean)
}
daply(data,.(sex,age),amean)
data
.parseISO8601('2000')
library(xts)
.parseISO8601('2000')
x <- timeBasedSeq('2010-01-01/2010-01-02 12:00')
x <- xts(1:length(x), x)
head(x)
indexClass(x)
indexFormat(x) <- "%Y-%b-%d %H:%M:%OS3"
head(x)
indexFormat(x) <- "%Y-%b-%d %H:%M:%OS3"
head(x)
x <- Sys.time() + 1:30
x
data(sample_matrix)
to.period(sample_matrix)
class(to.period(sample_matrix))
samplexts <- as.xts(sample_matrix)
to.period(samplexts)
class(to.period(samplexts))
endpoints(sample_matrix)
endpoints(sample_matrix, 'days',k=7)
endpoints(sample_matrix, 'weeks')
endpoints(sample_matrix, 'months')
(x <- xts(4:10, Sys.Date()+4:10))
(y <- xts(1:6, Sys.Date()+1:6))
merge(x,y)
merge(x,y, join='inner')
merge(x,y, join='left')
data(sample_matrix)
x <- as.xts(sample_matrix)
x
split(x)[[1]]
split(x)[[2]]
x <- xts(1:10, Sys.Date()+1:10)
x[c(1,2,5,9,10)] <- NA
x
na.locf(x)
na.locf(x, fromLast=TRUE)
xts.ts <- xts(rnorm(231),as.Date(13514:13744,origin="1970-01-01"))
start(xts.ts)
end(xts.ts)
zoo.data <- zoo(rnorm(31)+10,as.Date(13514:13744,origin="1970-01-01"))
ep <- endpoints(zoo.data,'weeks')
ep
timeBased(Sys.time())
timeBased(Sys.Date())
timeBased(200701)
timeBasedSeq('20080101 0830',length=100)
x <- xts(1:5, Sys.Date()+1:5)
x
lag(x)
library(RCurl)
headers = basicTextGatherer()
txt=getURL("http://www.dataguru.cn/",headerfunction = headers$update)
headers$value()
h = basicHeaderGatherer()
txtt=getURL("http://www.dataguru.cn/",headerfunction = h$update)
names(h$value())
h$value()
myheader <- c(
"User-Agent"="Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9.1.6) ",
"Accept"="text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
"Accept-Language"="en-us",
"Connection"="keep-alive",
"Accept-Charset"="GB2312,utf-8;q=0.7,*;q=0.7"
)
d = debugGatherer()
d$value()
temp <- getURL("http://www.dataguru.cn/",
httpheader = myheader,
debugfunction=d$update,verbose =TRUE)
cat(d$value()[3])#提交给服务器的头信息
temp <- getURL("http://www.dataguru.cn/",debugfunction=d$update,verbose =TRUE)
cat(d$value()[3])#提交给服务器的头信息
url.exists("http://cos.name/bbs/login.php?")
curl = getCurlHandle()
d=getURL("http://cos.name/cn/topic/411708/", curl = curl)
getCurlInfo(curl)$response.code
getCurlInfo(curl)
h = basicHeaderGatherer()
txtt=getURL("http://cos.name/cn/topic/411708/",headerfunction = h$update)
names(h$value())
h$value()
d$value()
getCurlInfo(curl)
headers = basicTextGatherer()
txt=getURL("http://www.dataguru.cn/",headerfunction = headers$update)
names(headers$value())#说明是字符串形式
headers$value()
wangye = "https://www.baidu.com/s?word=%E6%95%B0%E6%8D%AE%E7%82%BC%E9%87%91&tn=sitehao123&ie=utf-8&ssl_sample=hao_1"
getForm(wangye)
getFormParams(wangye)
wangye = "https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&tn=sitehao123&wd=RCurl&rsv_pq=da9cc0e900002b3e&rsv_t=259epf8EriexZ%2BBZqNgP0OLqTB6cvNAoPESiSn3z8LSnmaQ2jHMfVD76sLy%2F9v03Jg&rsv_enter=1&rsv_sug3=13&rsv_sug1=13&rsv_sug2=0&inputT=7148&rsv_sug4=7149"
getFormParams(wangye)
setwd("/media/zhoutao/软件盘/workspace/R/machine learning/04-Ranking")
library('tm')
library('ggplot2')
library('plyr')
# Set the global paths
#从上述路径中退回（..）到03-Classification目录中，然后进入data目录中
data.path <- file.path("..", "03-Classification", "data")
#在这个路劲中找到easy_ham目录
easyham.path <- file.path(data.path, "easy_ham")
# We define a set of function that will extract the data
# for the feature set we have defined to rank email
# impportance.  This includes the following: message
# body, message source, message subject, and date the
# message was sent.
# Simply returns the full text of a given email message
#打开该路径中的文件，将整封邮件作为一个字符串向量返回
msg.full <- function(path)
{
con <- file(path, open = "rt", encoding = "latin1")
msg <- readLines(con)
close(con)
return(msg)
}
# Retuns the email address of the sender for a given
# email message
#抽取特征1：抽取发件人的地址
#grepl与grep函数一样，用于匹配正则表达式模式串，不同的是后面的l代表（logical）
#表示它返回的不是向量索引号，而是返回与msg.vec长度一样的向量，这个向量中的元素用于
#标识字符串向量每个元素是否匹配上模式串
#用grepl先匹配到有（from：）的行
#strsplit将字一个字符串拆分成一个列表，用方括号为拆分模式创建一个字符集
#字符集中包含（冒号 尖括号 空格），那么第一个字符串为发件人地址
#因为文本模式存在变化，取[[1]] 有可能取不到，而是得到空格，所以我们要忽略空格
#然后查找包含@的元素并返回它们
get.from <- function(msg.vec)
{
from <- msg.vec[grepl("From: ", msg.vec)]
from <- strsplit(from, '[":<> ]')[[1]]
from <- from[which(from  != "" & from != " ")]
return(from[grepl("@", from)][1])
}
# Retuns the subject string for a given email message
#抽取特征2：邮件主题
#由于不是所有邮件都有主题，那么就要考察subj长度是否大于0
#大于0就把这一行按照模式拆分，并返回拆分得到的第二个元素
#否则返回空字符
get.subject <- function(msg.vec)
{
subj <- msg.vec[grepl("Subject: ", msg.vec)]
if(length(subj) > 0)
{
return(strsplit(subj, "Subject: ")[[1]][2])
}
else
{
return("")
}
}
# Similar to the function from Chapter 3, this returns
# only the message body for a given email.
#抽取特征3：邮件正文
get.msg <- function(msg.vec)
{
msg <- msg.vec[seq(which(msg.vec == "")[1] + 1, length(msg.vec), 1)]
return(paste(msg, collapse = "\n"))
}
# Retuns the date a given email message was received
#抽取特征4：收件箱邮件时间（处理时间相当困难）
#第一：不同编程语言对时间的设计理念不一样，R将日期字符转化为POSIX日期对象，然后对日期排序
#第二：邮件接收的日期和时间形式存在很大差异
#1 邮件时间中总是包含Date：，但是很多行匹配Date：
#2 日期时间不是同一的表达方式，但是所有邮件中都有一附加的格林尼治时间（GMT）偏移量
#在这里我么要剔除GMT抽取日期和时间
#通过观察发现只有一行所包含Date：在字符串首部，故用^Date完成
#只有在首部发现模式串时才返回TRUE
#因为时间总是在正文前面，如果正文中有一行首部也出现了Date：，为了保证不出错
#将第一个匹配上的元素作为返回对象
#日期和时间字符串得到后，要将他们转化为POSIX对象，
#首先要把日期和时间分离出来，用字符串拆分，从而标记多余信息
#分离出时间后，用gsub将首部或者尾部的空白字符替换掉
#标准的日期时间是25个字符，strtrim()将其后面的字符被裁减掉
get.date <- function(msg.vec)
{
date.grep <- grepl("^Date: ", msg.vec)
date.grep <- which(date.grep == TRUE)
date <- msg.vec[date.grep[1]]
date <- strsplit(date, "\\+|\\-|: ")[[1]][2]
date <- gsub("^\\s+|\\s+$", "", date)
return(strtrim(date, 25))
}
# This function ties all of the above helper functions together.
# It returns a vector of data containing the feature set
# used to categorize data as priority or normal HAM
#将邮件四种特征转化为结构化的数据方块
parse.email <- function(path)
{
full.msg <- msg.full(path)
date <- get.date(full.msg)
from <- get.from(full.msg)
subj <- get.subject(full.msg)
msg <- get.msg(full.msg)
return(c(date, from, subj, msg, path))
}
# In this case we are not interested in classifiying SPAM or HAM, so we will take
# it as given that is is being performed.  As such, we will use the EASY HAM email
# to train and test our ranker.
#得到目录中的文件名
easyham.docs <- dir(easyham.path)
#去掉cmds文件
easyham.docs <- easyham.docs[which(easyham.docs != "cmds")]
#对每一封邮件，抽取四种特征
easyham.parse <- lapply(easyham.docs,
function(p) parse.email(file.path(easyham.path, p)))
# Convert raw data from list to data frame
#将列表转化为数据框结构，将列表中的名称与对应的邮件特征按行排列
#也就是每一封邮件的特征排一行，
ehparse.matrix <- do.call(rbind, easyham.parse)
#将字符串改为因子
allparse.df <- data.frame(ehparse.matrix, stringsAsFactors = FALSE)
names(allparse.df) <- c("Date", "From.EMail", "Subject", "Message", "Path")
#head(allparse.df)
# Convert date strings to POSIX for comparison. Because the emails data
# contain slightly different date format pattners we have to account for
# this by passining them as required partmeters of the function.
#抽取日期所做的第一步只是简单的文本分类，现在要将文本转化为POSIX对象，以便逻辑比较
#由于日期时间格式不同，通过strptime()传入不同时间/日期格式来转化为POSIX对象
#不匹配的格式将返回NA, 因此，可以将第二个匹配的代替第一不匹配的NA
#这样就转化为相同格式的日期时间
#为了克服系统字符编码问题，增加了lct作为转换方法
date.converter <- function(dates, pattern1, pattern2)
{
lct <- Sys.getlocale("LC_TIME"); Sys.setlocale("LC_TIME", "C")
pattern1.convert <- strptime(dates, pattern1)
pattern2.convert <- strptime(dates, pattern2)
Sys.setlocale("LC_TIME", lct)
pattern1.convert[is.na(pattern1.convert)] <- pattern2.convert[is.na(pattern1.convert)]
return(pattern1.convert)
}
pattern1 <- "%a, %d %b %Y %H:%M:%S"
pattern2 <- "%d %b %Y %H:%M:%S"
allparse.df$Date <- date.converter(allparse.df$Date, pattern1, pattern2)
head(allparse.df$Date)
# Convert emails and subjects to lower-case
#将主题和发件人转化为小写，为的是格式尽量统一
allparse.df$Subject <- tolower(allparse.df$Subject)
allparse.df$From.EMail <- tolower(allparse.df$From.EMail)
# Order the messages chronologically
#按照时间对数据排序
#按照时间升序数据索引好排列
priority.df <- allparse.df[with(allparse.df, order(Date)), ]
#head(priority.df)
# We will use the first half of the priority.df to train our priority in-box algorithm.
# Later, we will use the second half to test.
#前半部分作为排序算法到训练集；后半部分作为测试集
priority.train <- priority.df[1:(round(nrow(priority.df) / 2)), ]
# The first step is to create rank weightings for all of the features.
# We begin with the simpliest: who the email is from.
# Calculate the frequency of correspondence with all emailers in the training set
#设计用于排序到权重计算
library('reshape2')
from.weight <- melt(with(priority.train, table(From.EMail)),
value.name="Freq")
from.weight <- from.weight[with(from.weight, order(Freq)), ]
from.weight <- ddply(priority.train, .(From.EMail), summary, Freq = length(Subjects))
from.weight <- ddply(priority.train, .(From.EMail), summarise, Freq = length(Subjects))
nemes(priority.train)
names(priority.train)
from.weight <- ddply(priority.train, .(From.EMail), summarise, Freq = length(Subject))
dim(priority.train)
from.weight
length(Subject)
length(priority.train$Subject)
from.weight <- ddply(priority.train, .(From.EMail), table, Freq = length(Subject))
length(priority.train$From.EMail)
ddply(priority.train, .(From.EMail), sum, Freq = length(Subject))
ddply(priority.train, From.EMail, summarise, Freq = length(Subject))
ddply(priority.train, .From.EMail, summarise, Freq = length(Subject))
ddply(priority.train, .(From.EMail), summarise, Freq = length(Subject))
from.weight <- from.weight[with(from.weight, order(Freq)), ]
from.ex <- subset(from.weight, Freq > 6)
from.scales <- ggplot(from.ex) +
geom_rect(aes(xmin = 1:nrow(from.ex) - 0.5,
xmax = 1:nrow(from.ex) + 0.5,
ymin = 0,
ymax = Freq,
fill = "lightgrey",
color = "darkblue")) +
scale_x_continuous(breaks = 1:nrow(from.ex), labels = from.ex$From.EMail) +
coord_flip() +
scale_fill_manual(values = c("lightgrey" = "lightgrey"), guide = "none") +
scale_color_manual(values = c("darkblue" = "darkblue"), guide = "none") +
ylab("Number of Emails Received (truncated at 6)") +
xlab("Sender Address") +
theme_bw() +
theme(axis.text.y = element_text(size = 5, hjust = 1))
from.scales
head(from.ex)
from.weight <- transform(from.weight,
Weight = log(Freq + 1),
log10Weight = log10(Freq + 1))
from.rescaled <- ggplot(from.weight, aes(x = 1:nrow(from.weight))) +
geom_line(aes(y = Weight, linetype = "ln")) +
geom_line(aes(y = log10Weight, linetype = "log10")) +
geom_line(aes(y = Freq, linetype = "Absolute")) +
scale_linetype_manual(values = c("ln" = 1,
"log10" = 2,
"Absolute" = 3),
name = "Scaling") +
xlab("") +
ylab("Number of emails Receieved") +
theme_bw() +
theme(axis.text.y = element_blank(), axis.text.x = element_blank())
from.rescaled
find.threads <- function(email.df)
{
response.threads <- strsplit(email.df$Subject, "re: ")
is.thread <- sapply(response.threads,
function(subj) ifelse(subj[1] == "", TRUE, FALSE))
threads <- response.threads[is.thread]
senders <- email.df$From.EMail[is.thread]
threads <- sapply(threads,
function(t) paste(t[2:length(t)], collapse = "re: "))
return(cbind(senders,threads))
}
threads.matrix <- find.threads(priority.train)
head(threads.matrix)
email.df = priority.train
response.threads <- strsplit(email.df$Subject, "re: ")
response.threads
head(threads.matrix)
email.thread <- function(threads.matrix)
{
senders <- threads.matrix[, 1]
senders.freq <- table(senders)
senders.matrix <- cbind(names(senders.freq),
senders.freq,
log(senders.freq + 1))
senders.df <- data.frame(senders.matrix, stringsAsFactors=FALSE)
row.names(senders.df) <- 1:nrow(senders.df)
names(senders.df) <- c("From.EMail", "Freq", "Weight")
senders.df$Freq <- as.numeric(senders.df$Freq)
senders.df$Weight <- as.numeric(senders.df$Weight)
return(senders.df)
}
senders.df <- email.thread(threads.matrix)
head(senders.df)
log(2)
thread.counts <- function(thread, email.df)
{
# Need to check that we are not looking at the original message in a thread,
# so we check the subjects against the 're:' cue.
thread.times <- email.df$Date[which(email.df$Subject == thread |
email.df$Subject == paste("re:", thread))]
freq <- length(thread.times)
min.time <- min(thread.times)
max.time <- max(thread.times)
time.span <- as.numeric(difftime(max.time, min.time, units = "secs"))
if(freq < 2)
{
return(c(NA, NA, NA))
}
else
{
trans.weight <- freq / time.span
log.trans.weight <- 10 + log(trans.weight, base = 10)
return(c(freq, time.span, log.trans.weight))
}
}
get.threads <- function(threads.matrix, email.df)
{
threads <- unique(threads.matrix[, 2])
thread.counts <- lapply(threads,
function(t) thread.counts(t, email.df))
thread.matrix <- do.call(rbind, thread.counts)
return(cbind(threads, thread.matrix))
}
thread.weights <- get.threads(threads.matrix, priority.train)
thread.weights <- get.threads(threads.matrix, priority.train)
thread.weights <- data.frame(thread.weights, stringsAsFactors = FALSE)
names(thread.weights) <- c("Thread", "Freq", "Response", "Weight")
thread.weights$Freq <- as.numeric(thread.weights$Freq)
thread.weights$Response <- as.numeric(thread.weights$Response)
thread.weights$Weight <- as.numeric(thread.weights$Weight)
thread.weights <- subset(thread.weights, is.na(thread.weights$Freq) == FALSE)
term.counts <- function(term.vec, control)
{
vec.corpus <- Corpus(VectorSource(term.vec))
vec.tdm <- TermDocumentMatrix(vec.corpus, control = control)
return(rowSums(as.matrix(vec.tdm)))
}
head(thread.weights)
thread.terms <- term.counts(thread.weights$Thread,
control = list(stopwords = TRUE))
thread.terms <- names(thread.terms)
thread.terms
term.weights <- sapply(thread.terms,
function(t) mean(thread.weights$Weight[grepl(t, thread.weights$Thread, fixed = TRUE)]))
msg.terms <- term.counts(priority.train$Message,
control = list(stopwords = TRUE,
removePunctuation = TRUE,
removeNumbers = TRUE))
msg.weights <- data.frame(list(Term = names(msg.terms),
Weight = log(msg.terms, base = 10)),
stringsAsFactors = FALSE,
row.names = 1:length(msg.terms))
msg.weights <- subset(msg.weights, Weight > 0)
get.weights <- function(search.term, weight.df, term = TRUE)
{
if(length(search.term) > 0)
{
if(term)
{
term.match <- match(names(search.term), weight.df$Term)
}
else
{
term.match <- match(search.term, weight.df$Thread)
}
match.weights <- weight.df$Weight[which(!is.na(term.match))]
if(length(match.weights) < 1)
{
return(1)
}
else
{
return(mean(match.weights))
}
}
else
{
return(1)
}
}
