# Set the global paths
#从上述路径中退回（..）到03-Classification目录中，然后进入data目录中
data.path <- file.path("..", "03-Classification", "data")
#在这个路劲中找到easy_ham目录
easyham.path <- file.path(data.path, "easy_ham")
# We define a set of function that will extract the data
# for the feature set we have defined to rank email
# impportance.  This includes the following: message
# body, message source, message subject, and date the
# message was sent.
# Simply returns the full text of a given email message
#打开该路径中的文件，将整封邮件作为一个字符串向量返回
msg.full <- function(path)
{
con <- file(path, open = "rt", encoding = "latin1")
msg <- readLines(con)
close(con)
return(msg)
}
# Retuns the email address of the sender for a given
# email message
#抽取特征1：抽取发件人的地址
#grepl与grep函数一样，用于匹配正则表达式模式串，不同的是后面的l代表（logical）
#表示它返回的不是向量索引号，而是返回与msg.vec长度一样的向量，这个向量中的元素用于
#标识字符串向量每个元素是否匹配上模式串
#用grepl先匹配到有（from：）的行
#strsplit将字一个字符串拆分成一个列表，用方括号为拆分模式创建一个字符集
#字符集中包含（冒号 尖括号 空格），那么第一个字符串为发件人地址
#因为文本模式存在变化，取[[1]] 有可能取不到，而是得到空格，所以我们要忽略空格
#然后查找包含@的元素并返回它们
get.from <- function(msg.vec)
{
from <- msg.vec[grepl("From: ", msg.vec)]
from <- strsplit(from, '[":<> ]')[[1]]
from <- from[which(from  != "" & from != " ")]
return(from[grepl("@", from)][1])
}
# Retuns the subject string for a given email message
#抽取特征2：邮件主题
#由于不是所有邮件都有主题，那么就要考察subj长度是否大于0
#大于0就把这一行按照模式拆分，并返回拆分得到的第二个元素
#否则返回空字符
get.subject <- function(msg.vec)
{
subj <- msg.vec[grepl("Subject: ", msg.vec)]
if(length(subj) > 0)
{
return(strsplit(subj, "Subject: ")[[1]][2])
}
else
{
return("")
}
}
# Similar to the function from Chapter 3, this returns
# only the message body for a given email.
#抽取特征3：邮件正文
get.msg <- function(msg.vec)
{
msg <- msg.vec[seq(which(msg.vec == "")[1] + 1, length(msg.vec), 1)]
return(paste(msg, collapse = "\n"))
}
# Retuns the date a given email message was received
#抽取特征4：收件箱邮件时间（处理时间相当困难）
#第一：不同编程语言对时间的设计理念不一样，R将日期字符转化为POSIX日期对象，然后对日期排序
#第二：邮件接收的日期和时间形式存在很大差异
#1 邮件时间中总是包含Date：，但是很多行匹配Date：
#2 日期时间不是同一的表达方式，但是所有邮件中都有一附加的格林尼治时间（GMT）偏移量
#在这里我么要剔除GMT抽取日期和时间
#通过观察发现只有一行所包含Date：在字符串首部，故用^Date完成
#只有在首部发现模式串时才返回TRUE
#因为时间总是在正文前面，如果正文中有一行首部也出现了Date：，为了保证不出错
#将第一个匹配上的元素作为返回对象
#日期和时间字符串得到后，要将他们转化为POSIX对象，
#首先要把日期和时间分离出来，用字符串拆分，从而标记多余信息
#分离出时间后，用gsub将首部或者尾部的空白字符替换掉
#标准的日期时间是25个字符，strtrim()将其后面的字符被裁减掉
get.date <- function(msg.vec)
{
date.grep <- grepl("^Date: ", msg.vec)
date.grep <- which(date.grep == TRUE)
date <- msg.vec[date.grep[1]]
date <- strsplit(date, "\\+|\\-|: ")[[1]][2]
date <- gsub("^\\s+|\\s+$", "", date)
return(strtrim(date, 25))
}
# This function ties all of the above helper functions together.
# It returns a vector of data containing the feature set
# used to categorize data as priority or normal HAM
#将邮件四种特征转化为结构化的数据方块
parse.email <- function(path)
{
full.msg <- msg.full(path)
date <- get.date(full.msg)
from <- get.from(full.msg)
subj <- get.subject(full.msg)
msg <- get.msg(full.msg)
return(c(date, from, subj, msg, path))
}
# In this case we are not interested in classifiying SPAM or HAM, so we will take
# it as given that is is being performed.  As such, we will use the EASY HAM email
# to train and test our ranker.
#得到目录中的文件名
easyham.docs <- dir(easyham.path)
#去掉cmds文件
easyham.docs <- easyham.docs[which(easyham.docs != "cmds")]
#对每一封邮件，抽取四种特征
easyham.parse <- lapply(easyham.docs,
function(p) parse.email(file.path(easyham.path, p)))
# Convert raw data from list to data frame
#将列表转化为数据框结构，将列表中的名称与对应的邮件特征按行排列
#也就是每一封邮件的特征排一行，
ehparse.matrix <- do.call(rbind, easyham.parse)
#将字符串改为因子
allparse.df <- data.frame(ehparse.matrix, stringsAsFactors = FALSE)
names(allparse.df) <- c("Date", "From.EMail", "Subject", "Message", "Path")
#head(allparse.df)
# Convert date strings to POSIX for comparison. Because the emails data
# contain slightly different date format pattners we have to account for
# this by passining them as required partmeters of the function.
#抽取日期所做的第一步只是简单的文本分类，现在要将文本转化为POSIX对象，以便逻辑比较
#由于日期时间格式不同，通过strptime()传入不同时间/日期格式来转化为POSIX对象
#不匹配的格式将返回NA, 因此，可以将第二个匹配的代替第一不匹配的NA
#这样就转化为相同格式的日期时间
#为了克服系统字符编码问题，增加了lct作为转换方法
date.converter <- function(dates, pattern1, pattern2)
{
lct <- Sys.getlocale("LC_TIME"); Sys.setlocale("LC_TIME", "C")
pattern1.convert <- strptime(dates, pattern1)
pattern2.convert <- strptime(dates, pattern2)
Sys.setlocale("LC_TIME", lct)
pattern1.convert[is.na(pattern1.convert)] <- pattern2.convert[is.na(pattern1.convert)]
return(pattern1.convert)
}
pattern1 <- "%a, %d %b %Y %H:%M:%S"
pattern2 <- "%d %b %Y %H:%M:%S"
allparse.df$Date <- date.converter(allparse.df$Date, pattern1, pattern2)
head(allparse.df$Date)
# Convert emails and subjects to lower-case
#将主题和发件人转化为小写，为的是格式尽量统一
allparse.df$Subject <- tolower(allparse.df$Subject)
allparse.df$From.EMail <- tolower(allparse.df$From.EMail)
# Order the messages chronologically
#按照时间对数据排序
#按照时间升序数据索引好排列
priority.df <- allparse.df[with(allparse.df, order(Date)), ]
#head(priority.df)
# We will use the first half of the priority.df to train our priority in-box algorithm.
# Later, we will use the second half to test.
#前半部分作为排序算法到训练集；后半部分作为测试集
priority.train <- priority.df[1:(round(nrow(priority.df) / 2)), ]
# The first step is to create rank weightings for all of the features.
# We begin with the simpliest: who the email is from.
# Calculate the frequency of correspondence with all emailers in the training set
#设计用于排序到权重计算
library('reshape2')
from.weight <- melt(with(priority.train, table(From.EMail)),
value.name="Freq")
from.weight <- from.weight[with(from.weight, order(Freq)), ]
from.weight <- ddply(priority.train, .(From.EMail), summary, Freq = length(Subjects))
from.weight <- ddply(priority.train, .(From.EMail), summarise, Freq = length(Subjects))
nemes(priority.train)
names(priority.train)
from.weight <- ddply(priority.train, .(From.EMail), summarise, Freq = length(Subject))
dim(priority.train)
from.weight
length(Subject)
length(priority.train$Subject)
from.weight <- ddply(priority.train, .(From.EMail), table, Freq = length(Subject))
length(priority.train$From.EMail)
ddply(priority.train, .(From.EMail), sum, Freq = length(Subject))
ddply(priority.train, From.EMail, summarise, Freq = length(Subject))
ddply(priority.train, .From.EMail, summarise, Freq = length(Subject))
ddply(priority.train, .(From.EMail), summarise, Freq = length(Subject))
from.weight <- from.weight[with(from.weight, order(Freq)), ]
from.ex <- subset(from.weight, Freq > 6)
from.scales <- ggplot(from.ex) +
geom_rect(aes(xmin = 1:nrow(from.ex) - 0.5,
xmax = 1:nrow(from.ex) + 0.5,
ymin = 0,
ymax = Freq,
fill = "lightgrey",
color = "darkblue")) +
scale_x_continuous(breaks = 1:nrow(from.ex), labels = from.ex$From.EMail) +
coord_flip() +
scale_fill_manual(values = c("lightgrey" = "lightgrey"), guide = "none") +
scale_color_manual(values = c("darkblue" = "darkblue"), guide = "none") +
ylab("Number of Emails Received (truncated at 6)") +
xlab("Sender Address") +
theme_bw() +
theme(axis.text.y = element_text(size = 5, hjust = 1))
from.scales
head(from.ex)
from.weight <- transform(from.weight,
Weight = log(Freq + 1),
log10Weight = log10(Freq + 1))
from.rescaled <- ggplot(from.weight, aes(x = 1:nrow(from.weight))) +
geom_line(aes(y = Weight, linetype = "ln")) +
geom_line(aes(y = log10Weight, linetype = "log10")) +
geom_line(aes(y = Freq, linetype = "Absolute")) +
scale_linetype_manual(values = c("ln" = 1,
"log10" = 2,
"Absolute" = 3),
name = "Scaling") +
xlab("") +
ylab("Number of emails Receieved") +
theme_bw() +
theme(axis.text.y = element_blank(), axis.text.x = element_blank())
from.rescaled
find.threads <- function(email.df)
{
response.threads <- strsplit(email.df$Subject, "re: ")
is.thread <- sapply(response.threads,
function(subj) ifelse(subj[1] == "", TRUE, FALSE))
threads <- response.threads[is.thread]
senders <- email.df$From.EMail[is.thread]
threads <- sapply(threads,
function(t) paste(t[2:length(t)], collapse = "re: "))
return(cbind(senders,threads))
}
threads.matrix <- find.threads(priority.train)
head(threads.matrix)
email.df = priority.train
response.threads <- strsplit(email.df$Subject, "re: ")
response.threads
head(threads.matrix)
email.thread <- function(threads.matrix)
{
senders <- threads.matrix[, 1]
senders.freq <- table(senders)
senders.matrix <- cbind(names(senders.freq),
senders.freq,
log(senders.freq + 1))
senders.df <- data.frame(senders.matrix, stringsAsFactors=FALSE)
row.names(senders.df) <- 1:nrow(senders.df)
names(senders.df) <- c("From.EMail", "Freq", "Weight")
senders.df$Freq <- as.numeric(senders.df$Freq)
senders.df$Weight <- as.numeric(senders.df$Weight)
return(senders.df)
}
senders.df <- email.thread(threads.matrix)
head(senders.df)
log(2)
thread.counts <- function(thread, email.df)
{
# Need to check that we are not looking at the original message in a thread,
# so we check the subjects against the 're:' cue.
thread.times <- email.df$Date[which(email.df$Subject == thread |
email.df$Subject == paste("re:", thread))]
freq <- length(thread.times)
min.time <- min(thread.times)
max.time <- max(thread.times)
time.span <- as.numeric(difftime(max.time, min.time, units = "secs"))
if(freq < 2)
{
return(c(NA, NA, NA))
}
else
{
trans.weight <- freq / time.span
log.trans.weight <- 10 + log(trans.weight, base = 10)
return(c(freq, time.span, log.trans.weight))
}
}
get.threads <- function(threads.matrix, email.df)
{
threads <- unique(threads.matrix[, 2])
thread.counts <- lapply(threads,
function(t) thread.counts(t, email.df))
thread.matrix <- do.call(rbind, thread.counts)
return(cbind(threads, thread.matrix))
}
thread.weights <- get.threads(threads.matrix, priority.train)
thread.weights <- get.threads(threads.matrix, priority.train)
thread.weights <- data.frame(thread.weights, stringsAsFactors = FALSE)
names(thread.weights) <- c("Thread", "Freq", "Response", "Weight")
thread.weights$Freq <- as.numeric(thread.weights$Freq)
thread.weights$Response <- as.numeric(thread.weights$Response)
thread.weights$Weight <- as.numeric(thread.weights$Weight)
thread.weights <- subset(thread.weights, is.na(thread.weights$Freq) == FALSE)
term.counts <- function(term.vec, control)
{
vec.corpus <- Corpus(VectorSource(term.vec))
vec.tdm <- TermDocumentMatrix(vec.corpus, control = control)
return(rowSums(as.matrix(vec.tdm)))
}
head(thread.weights)
thread.terms <- term.counts(thread.weights$Thread,
control = list(stopwords = TRUE))
thread.terms <- names(thread.terms)
thread.terms
term.weights <- sapply(thread.terms,
function(t) mean(thread.weights$Weight[grepl(t, thread.weights$Thread, fixed = TRUE)]))
msg.terms <- term.counts(priority.train$Message,
control = list(stopwords = TRUE,
removePunctuation = TRUE,
removeNumbers = TRUE))
msg.weights <- data.frame(list(Term = names(msg.terms),
Weight = log(msg.terms, base = 10)),
stringsAsFactors = FALSE,
row.names = 1:length(msg.terms))
msg.weights <- subset(msg.weights, Weight > 0)
get.weights <- function(search.term, weight.df, term = TRUE)
{
if(length(search.term) > 0)
{
if(term)
{
term.match <- match(names(search.term), weight.df$Term)
}
else
{
term.match <- match(search.term, weight.df$Thread)
}
match.weights <- weight.df$Weight[which(!is.na(term.match))]
if(length(match.weights) < 1)
{
return(1)
}
else
{
return(mean(match.weights))
}
}
else
{
return(1)
}
}
setwd("E:/workspace/R/machine learning/08-PCA")
prices <- read.csv(file.path('data', 'stock_prices.csv'),
stringsAsFactors = FALSE)
install.packages('lubridate')
library('lubridate')
prices[1, ]
prices <- transform(prices, Date = ymd(Date))
prices[1, ]
head(prices)
library('reshape')
date.stock.matrix <- cast(prices, Date ~ Stock, value = 'Close')
head(date.stock.matrix)
prices <- subset(prices, Date != ymd('2002-02-01'))
prices <- subset(prices, Stock != 'DDR')
date.stock.matrix <- cast(prices, Date ~ Stock, value = 'Close')
cor.matrix <- cor(date.stock.matrix[, 2:ncol(date.stock.matrix)])
correlations <- as.numeric(cor.matrix)
correlations
ggplot(data.frame(Correlation = correlations),
aes(x = Correlation, fill = 1)) +
geom_density() +
theme(legend.position = 'none')
library('ggplot2')
ggplot(data.frame(Correlation = correlations),
aes(x = Correlation, fill = 1)) +
geom_density() +
theme(legend.position = 'none')
data.frame(Correlation = correlations)
pca <- princomp(date.stock.matrix[, 2:ncol(date.stock.matrix)])
principal.component <- pca$loadings[, 1]
loadings <- as.numeric(principal.component)
ggplot(data.frame(Loading = loadings),
aes(x = Loading, fill = 1)) +
geom_density() +
theme(legend.position = 'none')
dji.prices <- read.csv(file.path('data', 'DJI.csv'),
stringsAsFactors = FALSE)
dji.prices <- transform(dji.prices, Date = ymd(Date))
# Twelfth code snippet
dji.prices <- subset(dji.prices, Date > ymd('2001-12-31'))
dji.prices <- subset(dji.prices, Date != ymd('2002-02-01'))
# Thirteenth code snippet
dji <- with(dji.prices, rev(Close))
dates <- with(dji.prices, rev(Date))
comparison <- data.frame(Date = dates,
MarketIndex = market.index,
DJI = dji)
market.index <- predict(pca)[, 1]
comparison <- data.frame(Date = dates,
MarketIndex = market.index,
DJI = dji)
ggplot(comparison, aes(x = MarketIndex, y = DJI)) +
geom_point() +
geom_smooth(method = 'lm', se = FALSE)
comparison <- transform(comparison, MarketIndex = -1 * MarketIndex)
comparison <- transform(comparison, MarketIndex = scale(MarketIndex))
comparison <- transform(comparison, DJI = scale(DJI))
alt.comparison <- melt(comparison, id.vars = 'Date')
head(alt.comparison)
alt.comparison
comparison
head(comparison)
names(alt.comparison) <- c('Date', 'Index', 'Price')
ggplot(alt.comparison, aes(x = Date, y = Price, group = Index, color = Index)) +
geom_point() +
geom_line()
data.file <- file.path('data', '01_heights_weights_genders.csv')
heights.weights <- read.csv(data.file, header = TRUE, sep = ',')
setwd("E:/workspace/R/machine learning/02-Exploration")
data.file <- file.path('data', '01_heights_weights_genders.csv')
heights.weights <- read.csv(data.file, header = TRUE, sep = ',')
ggplot(heights.weights, aes(x = Height)) +
geom_histogram(binwidth = 1)
ggplot(heights.weights, aes(x = Height)) +
geom_density()
ggplot(heights.weights, aes(x = Height, fill = Gender)) +
geom_density()
ggplot(heights.weights, aes(x = Weight, fill = Gender)) +
geom_density()
ggplot(heights.weights, aes(x = Weight, fill = Gender)) +
geom_density() +
facet_grid(Gender ~ .)
m <- 0
s <- 1
ggplot(data.frame(X = rnorm(100000, m, s)), aes(x = X)) +
geom_density()
set.seed(1)
normal.values <- rnorm(250, 0, 1)
cauchy.values <- rcauchy(250, 0, 1)
range(normal.values)
range(cauchy.values)
ggplot(data.frame(X = normal.values), aes(x = X)) +
geom_density()
ggplot(data.frame(X = cauchy.values), aes(x = X)) +
geom_density()
gamma.values <- rgamma(100000, 1, 0.001)
ggplot(data.frame(X = gamma.values), aes(x = X)) +
geom_density()
ggplot(heights.weights, aes(x = Height, y = Weight)) +
geom_point()
ggplot(heights.weights, aes(x = Height, y = Weight)) +
geom_point() +
geom_smooth()
ggplot(heights.weights[1:20, ], aes(x = Height, y = Weight)) +
geom_point() +
geom_smooth()
ggplot(heights.weights[1:200, ], aes(x = Height, y = Weight)) +
geom_point() +
geom_smooth()
ggplot(heights.weights[1:2000, ], aes(x = Height, y = Weight)) +
geom_point() +
geom_smooth()
ggplot(heights.weights, aes(x = Height, y = Weight)) +
geom_point(aes(color = Gender, alpha = 0.25)) +
scale_alpha(guide = "none") +
scale_color_manual(values = c("Male" = "black", "Female" = "gray")) +
theme_bw()
ggplot(heights.weights, aes(x = Height, y = Weight, color = Gender)) +
geom_point()
logit.model <- glm(Male ~ Weight + Height,
data = heights.weights,
family = binomial(link = 'logit'))
heights.weights <- transform(heights.weights,
Male = ifelse(Gender == 'Male', 1, 0))
logit.model <- glm(Male ~ Weight + Height,
data = heights.weights,
family = binomial(link = 'logit'))
ggplot(heights.weights, aes(x = Height, y = Weight)) +
geom_point(aes(color = Gender, alpha = 0.25)) +
scale_alpha(guide = "none") +
scale_color_manual(values = c("Male" = "black", "Female" = "gray")) +
theme_bw() +
stat_abline(intercept = -coef(logit.model)[1] / coef(logit.model)[2],
slope = - coef(logit.model)[3] / coef(logit.model)[2],
geom = 'abline',
color = 'black')
setwd("/media/zhoutao/软件盘/workspace/R/machine learning/05-Regression")
setwd("E:\workspace\R\machine learning\05-Regression")
setwd("E:/workspace/R/machine learning/05-Regression")
setwd("E:/workspace/R/machine learning/05-Regression")
library('ggplot2')
ages <- read.csv(file.path('data', 'longevity.csv'))
ggplot(ages, aes(x = AgeAtDeath, fill = factor(Smokes))) +
geom_density() +
facet_grid(Smokes ~ .)
head(ages)
ages <- read.csv(file.path('data', 'longevity.csv'))
guess <- 73
with(ages, mean((AgeAtDeath - guess) ^ 2))
mean(ages[,2])
with(ages, mean((AgeAtDeath - guess) ^ 2))
ages <- read.csv(file.path('data', 'longevity.csv'))
guess.accuracy <- data.frame()
for (guess in seq(63, 83, by = 1))
{
prediction.error <- with(ages,
mean((AgeAtDeath - guess) ^ 2))
#下面告诉我们如何在数据框中追加数据
guess.accuracy <- rbind(guess.accuracy,
data.frame(Guess = guess,
Error = prediction.error))
}
ggplot(guess.accuracy, aes(x = Guess, y = Error)) +
geom_point() +
geom_line()
ages <- read.csv(file.path('data', 'longevity.csv'))
constant.guess <- with(ages, mean(AgeAtDeath))
constant.guess
with(ages, sqrt(mean((AgeAtDeath - constant.guess) ^ 2)))
smokers.guess <- with(subset(ages, Smokes == 1),
mean(AgeAtDeath))
#不吸烟者平均寿命
non.smokers.guess <- with(subset(ages, Smokes == 0),
mean(AgeAtDeath))
smokers.guess
non.smokers.guess
ages <- transform(ages,
NewPrediction = ifelse(Smokes == 0,
non.smokers.guess,
smokers.guess))
head(ages)
with(ages, sqrt(mean((AgeAtDeath - NewPrediction) ^ 2)))
with(ages, sqrt(mean((AgeAtDeath - constant.guess) ^ 2)))
