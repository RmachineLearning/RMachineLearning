library(ggplot2)
ggplot(tophit,  aes(x=avg,  y=reorder(name,  avg))) +
geom_point(size=3) + # Use a larger dot
theme_bw() +
theme(panel.grid.major.x = element_blank(),
panel.grid.minor.x = element_blank(),
panel.grid.major.y = element_line(colour="grey60" ,  linetype="dashed" ))
install.packages("gcookbook")
tophit = tophitters2001[1:25, ]
library(gcookbook)
tophit = tophitters2001[1:25, ]
ggplot(tophit,  aes(x=avg,  y=reorder(name,  avg))) +
geom_point(size=3) + # Use a larger dot
theme_bw() +
theme(panel.grid.major.x = element_blank(),
panel.grid.minor.x = element_blank(),
panel.grid.major.y = element_line(colour="grey60" ,  linetype="dashed" ))
ggplot(tophit,  aes(x=avg,  y=reorder(name,  avg))) +
geom_point(size=3)
ggplot(tophit,  aes(x=avg,  y=reorder(name,  avg))) +
geom_point(size=3)
ggplot(tophit,  aes(x=avg,  y=reorder(name,  avg))) +
geom_point(size=3) + # Use a larger dot
theme_bw()
ggplot(tophit,  aes(x=avg,  y=reorder(name,  avg))) +
geom_point(size=3) + # Use a larger dot
theme_bw() +
theme(panel.grid.major.x = element_blank(),
panel.grid.minor.x = element_blank(),
panel.grid.major.y = element_line(colour="grey60" ,  linetype="dashed" ))
ggplot(tophit,  aes(x=avg,  y=reorder(name,  avg))) +
geom_point(size=3) + # Use a larger dot
theme_bw() +
theme(panel.grid.major.x = element_blank(),
panel.grid.minor.x = element_blank())
library(plyr)
tg <-  ddply(ToothGrowth,  c("supp" , "dose" ),  summarise,  length=mean(len))
tg
ToothGrowth
library(maptools)
China <- readShapePoly("E:\\workspace\\R\\画图\\地理画图\\全国地理文件\\中国地理总文件\\CHN_adm1.shp")
vent.map <-readShapeSpatial("E:\\workspace\\R\\画图\\地理画图\\全国地理文件\\中国地理总文件\\CHN_adm1.shp")
vent.map
install.packages("ff")
v = ff(vmode = "double", length = 1e+08)
library(ff)
v = ff(vmode = "double", length = 1e+08)
filename(v)
setwd("E:/workspace/R")
object.size(v)
file.info(filename(v) )$size
library(RCurl)
temp <- getURL("http://www.dataguru.cn/",debugfunction=d$update,verbose =TRUE)
d = debugGatherer()
temp <- getURL("http://www.dataguru.cn/",debugfunction=d$update,verbose =TRUE)
cat(d$value()[3])
cat(d$value()[1])
cat(d$value()[2])
headers = basicTextGatherer()
txt=getURL("http://www.dataguru.cn/",headerfunction = headers$update)
names(headers$value())#说明是字符串形式
headers$value()
h = basicHeaderGatherer()
txtt=getURL("http://www.dataguru.cn/",headerfunction = h$update)
names(h$value())
names(headers$value())
h$value()
curl = getCurlHandle()
d=getURL("http://www.dataguru.cn/", curl = curl)
getCurlInfo(curl)$response.code
getCurlInfo(curl)
url = getURL("http://rfunction.com/code/1202/")
temp<- getBinaryURL(url)
wp = getURL("http://www.bioguo.org/AnimalTFDB/BrowseAllTF.php?spe=Mus_musculus")
doc <- htmlParse(wp, asText = TRUE)
library(XML)
doc <- htmlParse(wp, asText = TRUE)
tables <- readHTMLTable(doc)
tables
head(tables)
str(tables)
tables$table1
head(tables$table1
)
head(tables$table1)
dizhen = getURL("http://data.earthquake.cn/datashare/datashare_more_quickdata_new.jsp")
dizhen_data <- getURL(dizhen)
doc <-htmlParse(wp, asText = TRUE)
tables <- readHTMLTable(doc,header=F)
head(tables)
head(tables$table1)
dizhen_doc <-htmlParse(dizhen, asText = TRUE)
dizhen_tables <- readHTMLTable(dizhen_doc,header=F)
head(dizhen_tables)
install.packages("RExcelInstaller", "rcom", "rsproxy")
require(devtools)
install.packages(devtools)
install.packages(“httr”)
install.packages("httr")
install.packages("devtools")
install_github("ramnathv/rCharts")
install_github("rcharts/ramnathv")
require(devtools)
find_rtools()
install.packages(c("abind", "adabag", "AER", "amap", "arules", "bayesm", "bdsmatrix", "BH", "boot", "bootstrap", "BradleyTerry2", "BRugs", "cairoDevice", "car", "caret", "caTools", "chron", "class", "classInt", "cluster", "coda", "colorspace", "corpcor", "corrgram", "coxme", "DCluster", "deldir", "DEoptimR", "deSolve", "diagram", "digest", "doBy", "dplyr", "dse", "e1071", "Ecdat", "Ecfun", "effects", "emplik", "evaluate", "fBasics", "fda", "fields", "forecast", "foreign", "formatR", "Formula", "gam", "gdata", "gee", "geoR", "geosphere", "GGally", "ggm", "ggmap", "ggplot2", "glmnet", "goftest", "GPArotation", "gplots", "gss", "gstat", "gtools", "gWidgetsRGtk2", "HH", "highr", "Hmisc", "htmltools", "httpuv", "igraph", "intervals", "ipred", "JM", "JMbayes", "kernlab", "KernSmooth", "kknn", "knitr", "kohonen", "labeling", "lava", "lme4", "lmtest", "manipulate", "mapdata", "mapproj", "maps", "maptools", "markdown", "MASS", "Matrix", "mboost", "mfp", "mime", "minqa", "mnormt", "MSBVAR", "multcomp", "mutoss", "mvoutlier", "nlme", "NLP", "nnet", "numDeriv", "party", "pcaPP", "pgirmess", "plotrix", "plspm", "plyr", "polyclip", "prodlim", "pscl", "psych", "pwr", "quantmod", "R.matlab", "R.methodsS3", "R.oo", "R.utils", "R2WinBUGS", "R6", "RandomFields", "randomForest", "RANN", "raster", "Rcmdr", "RColorBrewer", "Rcpp", "RcppArmadillo", "RcppEigen", "RCurl", "reshape2", "rgdal", "rgeos", "rgl", "RgoogleMaps", "rjson", "rmarkdown", "rminer", "robCompositions", "robustbase", "ROCR", "RODBC", "rpart", "rpart.plot", "rrcov", "sandwich", "scales", "sem", "seriation", "setRNG", "shape", "shiny", "sp", "spacetime", "spam", "SparseM", "spatial", "spatstat", "spBayes", "spdep", "sphet", "splancs", "splm", "stabledist", "statmod", "stringr", "strucchange", "survival", "systemfit", "tcltk2", "tfplot", "tframe", "TH.data", "timeDate", "timeSeries", "tm", "tsDyn", "tseries", "TSP", "TTR", "vcd", "VIM", "xlsxjars", "XML", "xtable", "zoo"))
install.packages(c("abind", "adabag", "AER", "amap", "arules",
install.packages(c("abind", "adabag", "AER", "amap", "arules", "bayesm", "bdsmatrix", "BH", "boot", "bootstrap", "BradleyTerry2", "BRugs", "cairoDevice", "car", "caret", "caTools", "chron", "class", "classInt", "cluster", "coda", "colorspace", "corpcor", "corrgram", "coxme", "DCluster", "deldir", "DEoptimR", "deSolve", "diagram", "digest", "doBy", "dplyr", "dse", "e1071", "Ecdat", "Ecfun", "effects", "emplik", "evaluate", "fBasics", "fda", "fields", "forecast", "foreign", "formatR", "Formula", "gam", "gdata", "gee", "geoR", "geosphere", "GGally", "ggm", "ggmap", "ggplot2", "glmnet", "goftest", "GPArotation", "gplots", "gss", "gstat", "gtools", "gWidgetsRGtk2", "HH", "highr", "Hmisc", "htmltools", "httpuv", "igraph", "intervals", "ipred", "JM", "JMbayes", "kernlab", "KernSmooth", "kknn", "knitr", "kohonen", "labeling", "lava", "lme4", "lmtest", "manipulate", "mapdata", "mapproj", "maps", "maptools", "markdown", "MASS", "Matrix", "mboost", "mfp", "mime", "minqa", "mnormt", "MSBVAR", "multcomp", "mutoss", "mvoutlier", "nlme", "NLP", "nnet", "numDeriv", "party", "pcaPP", "pgirmess", "plotrix", "plspm", "plyr", "polyclip", "prodlim", "pscl", "psych", "pwr", "quantmod", "R.matlab", "R.methodsS3", "R.oo", "R.utils", "R2WinBUGS", "R6", "RandomFields", "randomForest", "RANN", "raster", "Rcmdr", "RColorBrewer", "Rcpp", "RcppArmadillo", "RcppEigen", "RCurl", "reshape2", "rgdal", "rgeos", "rgl", "RgoogleMaps", "rjson", "rmarkdown", "rminer", "robCompositions", "robustbase", "ROCR", "RODBC", "rpart", "rpart.plot", "rrcov", "sandwich", "scales", "sem", "seriation", "setRNG", "shape", "shiny", "sp", "spacetime", "spam", "SparseM", "spatial", "spatstat", "spBayes", "spdep", "sphet", "splancs", "splm", "stabledist", "statmod", "stringr", "strucchange", "survival", "systemfit", "tcltk2", "tfplot", "tframe", "TH.data", "timeDate", "timeSeries", "tm", "tsDyn", "tseries", "TSP", "TTR", "vcd", "VIM", "xlsxjars", "XML", "xtable", "zoo"))
install.packages(c("abind", "adabag", "AER", "amap", "arules",
library("abind", lib.loc="E:/Program Files/R/R-3.2.1/library")
install.packages("dse")
install.packages("GPArotation")
install.packages(c("timeDate", "timeSeries", "tseries"))
library("dplyr", lib.loc="E:/Program Files/R/R-3.2.1/library")
lbrary(dplyr)
library(dplyr)
library("plyr", lib.loc="E:/Program Files/R/R-3.2.1/library")
xx<- array(1:24,c(3,4,2))
a<-array(1:21,c(3,7))
require(plyr)
library(plyr)
aaply(.data=a, .margins=1, .fun=mean)
a
xx
aaply(.data=a,  2, mean,  process="text")
names=c("john","mary","Alice","Peter","ROger","Phyillis")
age=c(13,15,14,13,14,13)
sex=c("M","F","F","M","M","F")
data=data.frame(names,age,sex)
aaply(.data=xx, .margins=1, .fun=mean)
aaply(.data=xx,  .margins = 2, .fun=mean,  process="text")
amean=function(data)
{
agemean=mean(data[,2])
return(agemean)
}
daply(data,.(sex,age),amean)
data
.parseISO8601('2000')
library(xts)
.parseISO8601('2000')
x <- timeBasedSeq('2010-01-01/2010-01-02 12:00')
x <- xts(1:length(x), x)
head(x)
indexClass(x)
indexFormat(x) <- "%Y-%b-%d %H:%M:%OS3"
head(x)
indexFormat(x) <- "%Y-%b-%d %H:%M:%OS3"
head(x)
x <- Sys.time() + 1:30
x
data(sample_matrix)
to.period(sample_matrix)
class(to.period(sample_matrix))
samplexts <- as.xts(sample_matrix)
to.period(samplexts)
class(to.period(samplexts))
endpoints(sample_matrix)
endpoints(sample_matrix, 'days',k=7)
endpoints(sample_matrix, 'weeks')
endpoints(sample_matrix, 'months')
(x <- xts(4:10, Sys.Date()+4:10))
(y <- xts(1:6, Sys.Date()+1:6))
merge(x,y)
merge(x,y, join='inner')
merge(x,y, join='left')
data(sample_matrix)
x <- as.xts(sample_matrix)
x
split(x)[[1]]
split(x)[[2]]
x <- xts(1:10, Sys.Date()+1:10)
x[c(1,2,5,9,10)] <- NA
x
na.locf(x)
na.locf(x, fromLast=TRUE)
xts.ts <- xts(rnorm(231),as.Date(13514:13744,origin="1970-01-01"))
start(xts.ts)
end(xts.ts)
zoo.data <- zoo(rnorm(31)+10,as.Date(13514:13744,origin="1970-01-01"))
ep <- endpoints(zoo.data,'weeks')
ep
timeBased(Sys.time())
timeBased(Sys.Date())
timeBased(200701)
timeBasedSeq('20080101 0830',length=100)
x <- xts(1:5, Sys.Date()+1:5)
x
lag(x)
library(RCurl)
headers = basicTextGatherer()
txt=getURL("http://www.dataguru.cn/",headerfunction = headers$update)
headers$value()
h = basicHeaderGatherer()
txtt=getURL("http://www.dataguru.cn/",headerfunction = h$update)
names(h$value())
h$value()
myheader <- c(
"User-Agent"="Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9.1.6) ",
"Accept"="text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
"Accept-Language"="en-us",
"Connection"="keep-alive",
"Accept-Charset"="GB2312,utf-8;q=0.7,*;q=0.7"
)
d = debugGatherer()
d$value()
temp <- getURL("http://www.dataguru.cn/",
httpheader = myheader,
debugfunction=d$update,verbose =TRUE)
cat(d$value()[3])#提交给服务器的头信息
temp <- getURL("http://www.dataguru.cn/",debugfunction=d$update,verbose =TRUE)
cat(d$value()[3])#提交给服务器的头信息
url.exists("http://cos.name/bbs/login.php?")
curl = getCurlHandle()
d=getURL("http://cos.name/cn/topic/411708/", curl = curl)
getCurlInfo(curl)$response.code
getCurlInfo(curl)
h = basicHeaderGatherer()
txtt=getURL("http://cos.name/cn/topic/411708/",headerfunction = h$update)
names(h$value())
h$value()
d$value()
getCurlInfo(curl)
headers = basicTextGatherer()
txt=getURL("http://www.dataguru.cn/",headerfunction = headers$update)
names(headers$value())#说明是字符串形式
headers$value()
wangye = "https://www.baidu.com/s?word=%E6%95%B0%E6%8D%AE%E7%82%BC%E9%87%91&tn=sitehao123&ie=utf-8&ssl_sample=hao_1"
getForm(wangye)
getFormParams(wangye)
wangye = "https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&tn=sitehao123&wd=RCurl&rsv_pq=da9cc0e900002b3e&rsv_t=259epf8EriexZ%2BBZqNgP0OLqTB6cvNAoPESiSn3z8LSnmaQ2jHMfVD76sLy%2F9v03Jg&rsv_enter=1&rsv_sug3=13&rsv_sug1=13&rsv_sug2=0&inputT=7148&rsv_sug4=7149"
getFormParams(wangye)
set.seed(1)
x <- seq(0, 1, by = 0.01)
y <- sin(2 * pi * x) + rnorm(length(x), 0, 0.1)
n <- length(x)
indices <- sort(sample(1:n, round(0.5 * n)))
training.x <- x[indices]
training.y <- y[indices]
test.x <- x[-indices]
test.y <- y[-indices]
df <- data.frame(X = x, Y = y)
training.df <- data.frame(X = training.x, Y = training.y)
test.df <- data.frame(X = test.x, Y = test.y)
rmse <- function(y, h)
{
return(sqrt(mean((y - h) ^ 2)))
}
# Twenty-second code snippet
library('glmnet')
glmnet.fit <- with(training.df, glmnet(poly(X, degree = 10), Y))
lambdas <- glmnet.fit$lambda
performance <- data.frame()
for (lambda in lambdas)
{
performance <- rbind(performance,
data.frame(Lambda = lambda,
RMSE = rmse(test.y,
with(test.df,
predict(glmnet.fit,
poly(X, degree = 10),
s = lambda)))))
}
# Twenty-third code snippet
ggplot(performance, aes(x = Lambda, y = RMSE)) +
geom_point() +
geom_line()
library('ggplot2')
ggplot(performance, aes(x = Lambda, y = RMSE)) +
geom_point() +
geom_line()
ggplot(performance, aes(x = Lambda, y = RMSE)) +
geom_point() +
geom_line() +
scale_x_log10()
best.lambda <- with(performance, Lambda[which(RMSE == min(RMSE))])
glmnet.fit <- with(df, glmnet(poly(X, degree = 10), Y))
# Twenty-fifth code snippet
coef(glmnet.fit, s = best.lambda)
ranks <- read.csv(file.path('data', 'oreilly.csv'),
stringsAsFactors = FALSE)
setwd("E:/workspace/R/machine learning/06-Regularization")
library('ggplot2')
ranks <- read.csv(file.path('data', 'oreilly.csv'),
stringsAsFactors = FALSE)
head(ranks)
library('tm')
head(ranks[,c(1,3)])
names(ranks)
library('tm')
documents <- data.frame(Text = ranks$Long.Desc.)
row.names(documents) <- 1:nrow(documents)
head(documents)
docs <- data.frame(c("This is a text.", "This another one."))
(ds <- DataframeSource(docs))
corpus
corpus <- Corpus(DataframeSource(documents))
corpus
#然后基于数据框创建一个语料库(corpus)，
corpus <- Corpus(DataframeSource(documents))
#tm_map()基于corpus进行文本转换，将文本中的单词全部统一成小写
corpus <- tm_map(corpus, tolower)
#去除多余空格
corpus <- tm_map(corpus, stripWhitespace)
#删除英语中常用的停用词
corpus <- tm_map(corpus, removeWords, stopwords('english'))
dtm <- DocumentTermMatrix(corpus)
documents <- data.frame(Text = ranks$Long.Desc.)
#每本书的描述，分别命名为1到最后
row.names(documents) <- 1:nrow(documents)
#DataframeSource()使数据框每一行代表一个文本
#然后基于数据框创建一个语料库(corpus)，
corpus <- Corpus(DataframeSource(documents))
#tm_map()基于corpus进行文本转换，将文本中的单词全部统一成小写
corpus <- tm_map(corpus, tolower)
#去除多余空格
corpus <- tm_map(corpus, stripWhitespace)
#删除英语中常用的停用词
corpus <- tm_map(corpus, removeWords, stopwords('english'))
dtm <- DocumentTermMatrix(corpus)
ranks <- read.csv(file.path('data', 'oreilly.csv'),
stringsAsFactors = FALSE)
#oreilly.csv包含的是oreilly出版社前100的畅销书，封底描述文本来预测这些书相对流行程度
#IP_Family 书地址 BOOK.title书名  BOOK.ISBN书号 Rank排名 Long.Desc 封底对书的描述
#
head(ranks[,c(1,3)])
names(ranks)
#正则化包，利用tm包将原始数据集转化成为一个文档词项矩阵
library('tm')
documents <- data.frame(Text = ranks$Long.Desc.)
#每本书的描述，分别命名为1到最后
row.names(documents) <- 1:nrow(documents)
#DataframeSource()使数据框每一行代表一个文本
#然后基于数据框创建一个语料库(corpus)，
corpus <- Corpus(DataframeSource(documents))
#tm_map()基于corpus进行文本转换，将文本中的单词全部统一成小写
corpus <- tm_map(corpus, tolower)
#去除多余空格
corpus <- tm_map(corpus, stripWhitespace)
#删除英语中常用的停用词
corpus <- tm_map(corpus, removeWords, stopwords('english'))
dtm <- DocumentTermMatrix(corpus)
DocumentTermMatrix
DocumentTermMatrix
dtm
summary(corpus)
corpus
type(corpus)
class(corpus)
dtm <- as.DocumentTermMatrix(corpus)
data("crude")
crude
dtm <- DocumentTermMatrix(crude,
control = list(weighting =
function(x)
weightTfIdf(x, normalize =
FALSE),
stopwords = TRUE))
dtm
dtm <- DocumentTermMatrix(corpus,
control = list(weighting =
function(x)
weightTfIdf(x, normalize =
FALSE),
stopwords = TRUE)))
dtm <- DocumentTermMatrix(corpus,
control = list(weighting =
function(x)
weightTfIdf(x, normalize =
FALSE),
stopwords = TRUE))
class(crude)\
class(crude)
TermDocumentMatrix(corpus)
corpus <- Corpus(DataframeSource(documents))
corpus <- tm_map(corpus, tolower)
corpus <- Corpus(DataframeSource(documents))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords('english'))
dtm <- DocumentTermMatrix(corpus)
head(dtm)
dtm
dtm1 <- DocumentTermMatrix(crude)
dtm1
x <- as.matrix(dtm)
y <- rev(1:100)
head(x)
x[1,1]
named(x)
names(x)
x
dtm
y <- rev(1:100)
library('glmnet')
performance <- data.frame()
for (lambda in c(0.1, 0.25, 0.5, 1, 2, 5))
{
for (i in 1:50)
{
indices <- sample(1:100, 80)
training.x <- x[indices, ]
training.y <- y[indices]
test.x <- x[-indices, ]
test.y <- y[-indices]
glm.fit <- glmnet(training.x, training.y)
predicted.y <- predict(glm.fit, test.x, s = lambda)
rmse <- sqrt(mean((predicted.y - test.y) ^ 2))
performance <- rbind(performance,
data.frame(Lambda = lambda,
Iteration = i,
RMSE = rmse))
}
}
ggplot(performance, aes(x = Lambda, y = RMSE)) +
stat_summary(fun.data = 'mean_cl_boot', geom = 'errorbar') +
stat_summary(fun.data = 'mean_cl_boot', geom = 'point')
y <- rep(c(1, 0), each = 50)
y
regularized.fit <- glmnet(x, y, family = 'binomial')
regularized.fit <- glmnet(x, y)
regularized.fit <- glmnet(x, y, family = 'gaussian')
regularized.fit <- glmnet(x, y, family = 'binomial')
predict(regularized.fit, newx = x, s = 0.001)
ifelse(predict(regularized.fit, newx = x, s = 0.001) > 0, 1, 0)
library('boot')
inv.logit(predict(regularized.fit, newx = x, s = 0.001))
set.seed(1)
performance <- data.frame()
for (i in 1:250)
{
indices <- sample(1:100, 80)
training.x <- x[indices, ]
training.y <- y[indices]
test.x <- x[-indices, ]
test.y <- y[-indices]
for (lambda in c(0.0001, 0.001, 0.0025, 0.005, 0.01, 0.025, 0.5, 0.1))
{
glm.fit <- glmnet(training.x, training.y, family = 'binomial')
predicted.y <- ifelse(predict(glm.fit, test.x, s = lambda) > 0, 1, 0)
error.rate <- mean(predicted.y != test.y)
performance <- rbind(performance,
data.frame(Lambda = lambda,
Iteration = i,
ErrorRate = error.rate))
}
}
